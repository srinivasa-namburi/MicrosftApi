{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on:\n",
        "[Industry Solutions Engineering LLM Data Ingestion](https://proud-glacier-0c2b6bb1e-1419.westus2.3.azurestaticapps.net/code-with-mlops/lab/llm-lab/module-2a/)\n",
        "\n",
        "Adapted for Project VICO by: Natasha Kohli"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Licensing Document exploration\n",
        "\n",
        "This notebook walks through an example exploratory data analysis workflow for exploring PDF documents. It is not a comprehensive overview of all relevant data science tasks that should happen prior to an AI engagement, but goes over several key phases in an LLM-focused project.\n",
        "\n",
        "## Setup\n",
        "Assuming this notebook is being run in an Azure ML notebook, please choose the \"SDK v2\" kernel, then run the first 2 cells below (with the pip installs). You should only need to run those once.\n",
        "\n",
        "Second, connect the SMR storage container to your Azure Machine Learning Workspace as a Datastore by following [official docs](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-connect-data-ui?view=azureml-api-1&tabs=credential#create-datastores).\n",
        "\n",
        "## Overview of Stages\n",
        "\n",
        "1. Use Document Intelligence (formerly Form Recognizer) to extract the content from the PDF\n",
        "    - Other tools could work just as well such as other 3rd party tools for PDF conversion to text or markdown\n",
        "2. Chunking the document into sizes that an embedding model can handle\n",
        "    - This is a critical part of the process, which splits the text of each document into \"chunks\" that fit within the token limit of embedding models. This notebook takes a single approach via LangChain, but to optimize the Project VICO solution, taking licensing document sections into account when chunking will likely be necessary.\n",
        "3. Use OpenAI Embedding model to create embedding vectors of the chunks\n",
        "    - These embeddings can be used to represent the content, and then the vectors can be used for search\n",
        "4. Use PCA for dimensionality reduction of the embedding vector, and K-means to cluster\n",
        "    - Allows data scientists to visualize and explore trends within the data\n",
        "5. Visualize the clusters using plotting tools\n",
        "6. Create a Cognitive Search Index using embedding vectors\n",
        "    - Enable Vector Search and Hybrid Search in Azure Cognitive Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1692737778830
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%pip install azure-search-documents==11.4.0b8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1692725868667
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%pip install azure-ai-formrecognizer==3.3.0 azureml-fsspec==1.2.0 mltable==1.5.0 tenacity==8.2.3 openai==0.28.0 langchain==0.0.281 tiktoken==0.4.0 plotly==5.16.1 spacy==3.6.1 nbformat==5.9.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694818458611
        }
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from typing import List, Union, IO\n",
        "\n",
        "import numpy as np\n",
        "import openai\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import spacy\n",
        "from azure.ai.formrecognizer import AnalyzeResult, DocumentAnalysisClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.search.documents import SearchClient\n",
        "from azure.search.documents.indexes import SearchIndexClient\n",
        "from azure.search.documents.indexes.models import (\n",
        "    HnswVectorSearchAlgorithmConfiguration,\n",
        "    PrioritizedFields,\n",
        "    SearchableField,\n",
        "    SearchField,\n",
        "    SearchFieldDataType,\n",
        "    SearchIndex,\n",
        "    SemanticConfiguration,\n",
        "    SemanticField,\n",
        "    SemanticSettings,\n",
        "    SimpleField,\n",
        "    VectorSearch,\n",
        ")\n",
        "from azure.search.documents.models import Vector\n",
        "from azureml.fsspec import AzureMachineLearningFileSystem\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from tenacity import retry, stop_after_attempt, wait_random_exponential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694815189602
        },
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# only run this once, then comment it out even if you restart your kernel (unless you restart the whole computer)\n",
        "spacy.cli.download(\"en_core_web_lg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load the data for chunking via Document Intelligence Service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694818461281
        }
      },
      "outputs": [],
      "source": [
        "# Set your subscription, resource group and workspace name:\n",
        "subscription_id = \"SUBSCRIPTION_ID\"\n",
        "resource_group = \"RESOURCE_GROUP\"\n",
        "workspace = \"WORKSPACE_NAME\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the Azure Machine Learning File System tools to mount our datastore to the compute instance and load data directly. By default, we are using the full licensing document PDFs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694818468268
        }
      },
      "outputs": [],
      "source": [
        "datastore_name = \"licensingpdfs\"\n",
        "datastore_uri = f\"azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/{datastore_name}\"\n",
        "fs = AzureMachineLearningFileSystem(datastore_uri)\n",
        "documents = fs.glob(\"pdf/*.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional cell - If you ran into auth issues above due to a Mac OS bug:\n",
        "\n",
        "# Manually get the names of the documents\n",
        "# documents = [\"Aurora Environmental Report_ML20075A004.pdf\", \"Environmental Impact Kairos Hermes_ML22259A126.pdf\", \"Hermes Non-Power Reactor_ML21306A133.pdf\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1694818470665
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694818474494
        }
      },
      "outputs": [],
      "source": [
        "# An important dataframe saves the path and the content of all documents\n",
        "document_df = pd.DataFrame({\"FileName\": documents})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Document Cracking\n",
        "Document Intelligence (formerly Form Recognizer) has good PDF cracking capabilities. We won't be using all of the tools at our disposal here, but just pulling the text out of the PDFs. This notebook uses the [General Document Model](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept-general-document?view=doc-intel-3.1.0) by default.\n",
        "\n",
        "The `tenacity` library is used for intelligent retrying of our method, in the case of failures. The `@retry` decorator at the top of the method defines the retry logic.\n",
        "\n",
        "If you would like to, feel free to try a different toolset for retrieving the text in the PDF documents or tools to convert PDF to other file types. Perhaps you can find a more performant option!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694818477896
        }
      },
      "outputs": [],
      "source": [
        "endpoint = \"https://<resourcename>.cognitiveservices.azure.com/\"\n",
        "credential = AzureKeyCredential(\"DOCUMENT INTELLIGENCE KEY\")\n",
        "document_analysis_client = DocumentAnalysisClient(endpoint, credential)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1694818480151
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\n",
        "def get_content(\n",
        "    document: Union[bytes, IO[bytes]], model_id: str = \"prebuilt-document\"\n",
        ") -> AnalyzeResult:\n",
        "    \"\"\"\n",
        "    Analyze field text from a given document.\n",
        "\n",
        "    Args:\n",
        "        document (Union[bytes, IO[bytes]]): the document that is going to be analyzed\n",
        "        model_id (str, optional): a unique model identifier. Defaults to \"prebuilt-document\":str.\n",
        "            Please see here for more details.\n",
        "            https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept-model-overview?view=doc-intel-3.1.0#model-data-extraction\n",
        "\n",
        "    Returns:\n",
        "        AnalyzeResult: the analyzed results\n",
        "    \"\"\"\n",
        "    poller = document_analysis_client.begin_analyze_document(\n",
        "        model_id=model_id, document=document\n",
        "    )\n",
        "    result = poller.result()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# WAIT - Before running next cells\n",
        "Running Document Intelligence across a large amount of documents (in the hundreds) can take over an hour. Consider trying this out on a smaller subset, and taking advantage of the pre-extracted embeddings/documents for the later modules to access the full dataset.\n",
        "\n",
        "By default, the code in the above cells references a folder with 3 documents which should take less than 5 minutes to run.\n",
        "\n",
        "If you would like to run a full dataset in the future, you can change the folder referenced in the above cells. This will require more data cleansing, more data prep, and of course much more time and is not recommended when first running through the lab, or when operating under a fixed timetable (such as a hackathon)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694818506675
        }
      },
      "outputs": [],
      "source": [
        "# Recommended path - instead of running this notebook on a full dataset, use a subset to explore\n",
        "for index, row in document_df.iterrows():\n",
        "    full_path = f\"{datastore_uri}/paths/{row['FileName']}\"\n",
        "    print(full_path)\n",
        "    with fs.open(full_path, \"rb\") as f:\n",
        "        print(f\"Analyzing {row['FileName']}\")\n",
        "        result = get_content(document=f)\n",
        "        filecontent = result.content\n",
        "        # Optionally, save the full DI response to json in case you want to explore that data later. E.g. page numbers, paragraphs\n",
        "        # json.dump(result.to_dict(), open(f\"data/{row['FileName']}.json\", \"w\"))\n",
        "    document_df.loc[index, \"Content\"] = filecontent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694818508098
        }
      },
      "outputs": [],
      "source": [
        "document_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Troubleshooting Document Intelligence\n",
        "\n",
        "We have seen times where the DI service times out when running large numbers of docs through it. The next cell's code looks for files that did not have results returned. If you run into a lot of documents that didn't get parsed, use this list to re-run the subset of docs back through the service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1692681723437
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# documents = document_df.loc[document_df['Content'].isnull() == True]['FileName']\n",
        "# documents = documents.to_list()\n",
        "# documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694816508113
        }
      },
      "outputs": [],
      "source": [
        "# drop the rows if the content is null - many will be since we only evaluated a subset\n",
        "print(document_df.shape)\n",
        "document_df = document_df.dropna(subset=[\"Content\"])\n",
        "print(document_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Post Processing\n",
        "\n",
        "PDF files can sometimes be corrupted, blank, or read-protected. Resolving these issues is out of scope for this hack, so we'll drop them from our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1692681947907
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Also drop any empty files\n",
        "document_df = document_df.dropna()\n",
        "# Drop files that had a processing error (may be corrupted)\n",
        "document_df = document_df[\n",
        "    ~document_df.Content.str.contains(\"This file cannot be downloaded\")\n",
        "]\n",
        "# Drop protected files (out of scope for fixing)\n",
        "document_df = document_df[\n",
        "    ~document_df.Content.str.contains(\"This PDF file is protected\")\n",
        "]\n",
        "print(document_df.shape)\n",
        "document_df = document_df.reset_index(drop=True)\n",
        "# Optional: Save this checkpoint to parquet\n",
        "document_df.to_parquet(\"extracted_pdfs.gzip\", compression=\"gzip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694818524870
        }
      },
      "outputs": [],
      "source": [
        "document_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1692814689616
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Optional Cell - load the extracted text. Useful for if you had to restart your notebook\n",
        "\n",
        "# checkpoint\n",
        "document_df = pd.read_parquet(\"extracted_pdfs.gzip\")\n",
        "document_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Chunking\n",
        "Now, we have all of our data as text, we need to chunk it into sizes that an embedding model can handle (e.g. < 8192 tokens, the limit for the Ada model).\n",
        "\n",
        "Below, we are using [LangChain's token based splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/split_by_token), which uses the `tiktoken` library to intelligently break up the text into our specified sizes. Feel free to try different chunk sizes, and overlap amounts (how much of the previous chunk is in the next one), to see if the final results of the notebook have improved. Document chunking is an important part of an LLM solution, balancing between including enough context in a given chunk, while keeping them small enough to be return in relevant search queries.\n",
        "\n",
        "If you encounter permission problems when using `tiktoken` due to cache location, try to set the environment variable\n",
        "```export TIKTOKEN_CACHE_DIR=\"./YOUR_TIKTOKEN_CACHE_DIR\" ```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we dive all the way into chunking every document, let's explore what the results are for a single document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1694816567585
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "test_content = document_df.loc[0, \"Content\"] # test on the first document's content\n",
        "text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=15)\n",
        "texts = text_splitter.split_text(test_content)\n",
        "\n",
        "len(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "We can now review the different chunks that were split in a single document, before splitting the rest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1694816573487
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Now that we've seen how chunking works on a single document, let's run it against the rest of our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1694818541851
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# the dataframe used for saving the chunked content\n",
        "chunked_df = []\n",
        "text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=50)\n",
        "\n",
        "for index, row in document_df.iterrows():\n",
        "    # print(f\"Splitting {row['FileName']}\")\n",
        "    if isinstance(row[\"Content\"], str):\n",
        "        texts = text_splitter.split_text(row[\"Content\"])\n",
        "        # Clean up the newline content\n",
        "        cleaned_texts = []\n",
        "        for text in texts:\n",
        "            text = text.strip().replace(\"\\n\", \" \")\n",
        "            cleaned_texts.append(text)\n",
        "        chunked_df.append(\n",
        "            pd.DataFrame({\"FileName\": row[\"FileName\"], \"Content\": cleaned_texts})\n",
        "        )\n",
        "chunked_df = pd.concat(chunked_df, ignore_index=True)\n",
        "# Optional: Save this checkpoint to parquet\n",
        "chunked_df.to_parquet(\"chunked_data.gzip\", compression=\"gzip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694818550711
        }
      },
      "outputs": [],
      "source": [
        "chunked_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694818554138
        }
      },
      "outputs": [],
      "source": [
        "chunked_df.loc[0, \"Content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional Cell - load the chunked data. Useful for if you had to restart your notebook\n",
        "\n",
        "# checkpoint\n",
        "chunked_df = pd.read_parquet(\"chunked_data.gzip\")\n",
        "chunked_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Generate Embeddings\n",
        "\n",
        "Here we define how to use Azure OpenAI for [generating embeddings](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/embeddings?tabs=console) (at bare minimum, we use this for our queries at the end). Once again, the `tenacity` library is used for intelligent retries against the service to avoid failures due to timeout.\n",
        "\n",
        "For this to work, you need to have an OpenAI service set up in your resource group, and have deployed a `text-embedding-ada-002` model. That said, if you want to try a different embedding model from somewhere like HuggingFace, feel free to explore.\n",
        "\n",
        "Be mindful of the quota limits of embeddings and how large your set of chunked documents is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694819355369
        }
      },
      "outputs": [],
      "source": [
        "# OpenAI deployment values\n",
        "openai_api_key = \"OPENAI API KEY\"\n",
        "openai_api_type = \"azure\"\n",
        "openai_api_base = \"https://OPENAI API ENDPOINT.openai.azure.com/\"\n",
        "openai_api_version = \"2023-06-01-preview\"\n",
        "openai_deployment_id = \"EMBEDDING DEPLOYMENT NAME\"\n",
        "\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
        "def generate_embeddings(text: str) -> List[float]:\n",
        "    \"\"\"Generate embdding for string\n",
        "\n",
        "    Args:\n",
        "        text (str): string for embedding\n",
        "\n",
        "    Returns:\n",
        "        List[float]: the embedding for the text\n",
        "    \"\"\"\n",
        "    openai.api_key = openai_api_key\n",
        "    openai.api_type = openai_api_type\n",
        "    openai.api_base = openai_api_base\n",
        "    openai.api_version = openai_api_version\n",
        "    response = openai.Embedding.create(input=text, deployment_id=openai_deployment_id)\n",
        "    embeddings = response[\"data\"][0][\"embedding\"]\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "If you'd like to see how it works, try running it on a single row of the dataframe, or on your smaller test dataset.\n",
        "Running it on the full chunked dataset can take anywhere from 15 minutes to over an hour depending on the rate limits on your service."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1694816806293
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "embedding_df = chunked_df.copy()\n",
        "embedding = generate_embeddings(embedding_df[\"Content\"][0])\n",
        "embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Here, we use a lambda function to run it across our small test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694816822967
        },
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "embedding_df[\"Generated_embedding\"] = embedding_df[\"Content\"].apply(\n",
        "    lambda x: generate_embeddings(x)\n",
        ")\n",
        "embedding_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1694816827986
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Checkpoint - save the data to parquet so that you don't lose it if your notebook needs to restart.\n",
        "embedding_df.to_parquet(\"embedded_content.gzip\", compression=\"gzip\")\n",
        "\n",
        "eda_df = embedding_df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Checkpoint: Load the parquet\n",
        "If your notebook is still running, you don't need this cell, just check that the variable name of your dataframe is correct moving forward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694818575244
        }
      },
      "outputs": [],
      "source": [
        "# Optional Cell - load the chunked data. Useful for if you had to restart your notebook\n",
        "\n",
        "# checkpoint\n",
        "eda_df = pd.read_parquet(\"embedded_content.gzip\")\n",
        "# check to see if it loaded correctly\n",
        "eda_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Optional: Generate Keywords on content\n",
        "\n",
        "If you wish, you can add keywords as metadata to your dataset by using the `Spacy` natural language tools. This takes a few minutes to run, but can be helpful in improving your search index by adding keywords to search across. Here we use the Named Entity Recognition built into the tool to identify these keywords. For more information on NER, spaCy has a great [video here](https://spacy.io/universe/project/video-spacys-ner-model-alt)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1692817773190
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "lines_to_next_cell": 2,
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# here, we load the spacy model we're going to use. The code to download it is\n",
        "# near the top of this notebook\n",
        "nlp = spacy.load(\"en_core_web_lg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1692819727642
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def ner(text: str) -> List[str]:\n",
        "    \"\"\"Generate keywords from the text\n",
        "\n",
        "    Args:\n",
        "        text (str): the text you want to generate keywords\n",
        "\n",
        "    Returns:\n",
        "        List[str]: keywords\n",
        "    \"\"\"\n",
        "\n",
        "    entities = []\n",
        "    for entity in nlp(text).ents:\n",
        "        entities.append(entity.text)\n",
        "    return entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1692820640206
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "eda_df[\"keywords\"] = eda_df[\"Content\"].apply(ner)\n",
        "eda_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EDA\n",
        "\n",
        "[PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) is used to reduce the dimentionality of the embeddings, to eventually cluster them. In this notebook, we run in 3 steps: PCA to ~20 values to reduce dimensionality, [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) cluster on that reduced dataset, and then PCA one more time to use as X and Y values for plotting.\n",
        "\n",
        "This series of steps has generally resulted in more \"interesting\" clusters to explore later, but this is an area for you to explore. Consider changes such as only doing the PCA reduction 1 time and clustering/plotting that - or changing the number of clusters. This is the *Exploratory* data analysis after all."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694818588177
        }
      },
      "outputs": [],
      "source": [
        "# Pandas and Numpy arrays don't directly play nicely together, so we cast back to list\n",
        "eda_df[\"PCA_1\"] = (\n",
        "    PCA(n_components=20).fit_transform(eda_df[\"Generated_embedding\"].tolist()).tolist()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have a more reasonably dimensioned set of embeddings, we'll create clusters (grouping the data together)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694818592269
        }
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(init=\"k-means++\", n_clusters=40, n_init=\"auto\")\n",
        "kmeans.fit(eda_df[\"PCA_1\"].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694818594169
        }
      },
      "outputs": [],
      "source": [
        "eda_df[\"Cluster\"] = kmeans.predict(eda_df[\"PCA_1\"].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694818595533
        },
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "eda_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For plotting those clusters, it's easier if we then have a 2-dimensional representation of the embeddings, so we'll run PCA again to reduce down to 2 dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694818599809
        },
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "eda_df[\"PCA\"] = (\n",
        "    PCA(n_components=2).fit_transform(eda_df[\"Generated_embedding\"].tolist()).tolist()\n",
        ")\n",
        "eda_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plotting the data\n",
        "\n",
        "Using [Plotly](https://plotly.com/python/line-and-scatter/), we can create a scatterplot (or [seaborn](https://seaborn.pydata.org/), or [matplotlib](https://matplotlib.org/)). Plotly allows for more interactive plots, which is why it's used here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1694818603522
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Use our 2-D PCA values as X and Y coordinates for plotting.\n",
        "eda_df[\"X\"] = eda_df[\"PCA\"].str[0]\n",
        "eda_df[\"Y\"] = eda_df[\"PCA\"].str[1]\n",
        "eda_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694816907104
        }
      },
      "outputs": [],
      "source": [
        "px.scatter(eda_df, x=\"X\", y=\"Y\", color=\"Cluster\", hover_data=\"FileName\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The tradeoff of using PCA 2 times is that the clusters aren't as visually distinctly plotted - vs creating the clusters from a 2-D vector. But, the clusters seemed to be better differentiated in terms of content.\n",
        "\n",
        "We can filter our dataframe to look at a given cluster to try and see what's being linked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694818613209
        }
      },
      "outputs": [],
      "source": [
        "filter_df = eda_df[eda_df[\"Cluster\"] == 44].head(10)\n",
        "# If you generated keywords:\n",
        "# filter_df[[\"FileName\", \"Cluster\", \"Content\", \"keywords\"]]\n",
        "\n",
        "# Otherwise:\n",
        "filter_df[[\"FileName\", \"Cluster\", \"Content\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's prepare our Search index. For this, we want to use the FULL dataset, so first, download the pre-prepared CSV. The code below shows the last step that was performed on the full dataset prior to being saved (creating an `id` column, and filtering down to the 4 other primary columns), and is for informational purposes only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694818944439
        }
      },
      "outputs": [],
      "source": [
        "# lets drop some of the columns we won't be using for searching. Optionally leaving keywords\n",
        "# search_df = eda_df.loc[:, [\"FileName\", \"Content\", \"Generated_embedding\", \"keywords\"]]\n",
        "search_df = eda_df.loc[:, [\"FileName\", \"Content\", \"Generated_embedding\"]]\n",
        "search_df[\"id\"] = range(len(eda_df))\n",
        "search_df[\"id\"] = search_df[\"id\"].map(str)\n",
        "\n",
        "search_df.to_parquet(\"searchdf.gzip\", compression=\"gzip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Create Search Index\n",
        "\n",
        "Using our processed data, we can now create an Azure Cognitive Search Index to query against."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694819028957
        }
      },
      "outputs": [],
      "source": [
        "search_df = pd.read_parquet(\"searchdf.gzip\")\n",
        "search_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Index Configuration\n",
        "\n",
        "Fields:\n",
        "\n",
        "- SimpleField: Used for faceting, filtering, or sorting results. Can be searchable, but not full text searchable. In this case, we are using only 1, our \"id\" field, and setting it as the key for the index.\n",
        "- SearchableFields are full text searchable (they can also be set to be used for filtering, faceting, and sorting).\n",
        "  - Here, we are passing in the Content, keywords, and FileNames as Strings.\n",
        "  - Generated_embedding is added as a collection of datapoints. This is the field that we will be using for our vector search.\n",
        "\n",
        "Configurations:\n",
        "\n",
        "- Vector Search: Configuration for the Vector Search is primarily static - most of the values in this configuration are hard-coded at this time, aside from the name you wish to set for the configuration. This is still in early enough preview that the documentation is not fully available for python. The REST API details are available [here](https://learn.microsoft.com/en-us/rest/api/searchservice/preview-api/create-or-update-index#request-body).\n",
        "- Semantic Search: Configuration for the Semantic Search defines which fields in the index will be used for semantic queries. For more info, review the [Quickstart Guide](https://learn.microsoft.com/en-us/azure/search/search-get-started-semantic?tabs=python#add-semantic-search).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694819056559
        }
      },
      "outputs": [],
      "source": [
        "# Create a search index\n",
        "credential = AzureKeyCredential(\"SEARCH ADMIN KEY\")\n",
        "service_endpoint = \"https://SEARCH ENDPOINT.search.windows.net\"\n",
        "index_name = \"CREATE INDEX NAME\"\n",
        "index_client = SearchIndexClient(endpoint=service_endpoint, credential=credential)\n",
        "fields = [\n",
        "    SimpleField(\n",
        "        name=\"id\",\n",
        "        type=SearchFieldDataType.String,\n",
        "        key=True,\n",
        "        sortable=True,\n",
        "        filterable=True,\n",
        "        facetable=True,\n",
        "    ),\n",
        "    SearchableField(name=\"FileName\", type=SearchFieldDataType.String),\n",
        "    SearchableField(name=\"Content\", type=SearchFieldDataType.String),\n",
        "    SearchField(\n",
        "        name=\"Generated_embedding\",\n",
        "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
        "        searchable=True,\n",
        "        vector_search_dimensions=1536,\n",
        "        vector_search_configuration=\"my-vector-config\",\n",
        "    ),\n",
        "    # if you generated keywords\n",
        "    # SearchField(\n",
        "    #     name=\"keywords\", type=SearchFieldDataType.Collection(SearchFieldDataType.String)\n",
        "    # ),\n",
        "]\n",
        "\n",
        "# These values don't seem to be configurable\n",
        "vector_search = VectorSearch(\n",
        "    algorithm_configurations=[\n",
        "        HnswVectorSearchAlgorithmConfiguration(\n",
        "            name=\"my-vector-config\",\n",
        "            kind=\"hnsw\",\n",
        "            parameters={\n",
        "                \"m\": 4,\n",
        "                \"efConstruction\": 400,\n",
        "                \"efSearch\": 500,\n",
        "                \"metric\": \"cosine\",\n",
        "            },\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "semantic_config = SemanticConfiguration(\n",
        "    name=\"my-semantic-config\",\n",
        "    prioritized_fields=PrioritizedFields(\n",
        "        title_field=SemanticField(field_name=\"FileName\"),\n",
        "        prioritized_content_fields=[SemanticField(field_name=\"Content\")],\n",
        "        # if you generated keywords\n",
        "        # prioritized_keywords_fields=[SemanticField(field_name=\"keywords\")],\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Create the semantic settings with the configuration\n",
        "semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
        "\n",
        "# Create the search index with the semantic settings\n",
        "index = SearchIndex(\n",
        "    name=index_name,\n",
        "    fields=fields,\n",
        "    vector_search=vector_search,\n",
        "    semantic_settings=semantic_settings,\n",
        ")\n",
        "result = index_client.create_or_update_index(index)\n",
        "print(f\"{result.name} created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Azure Search expects JSON for creating an index, so convert our index to JSON. This also lets us save our work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694819064711
        }
      },
      "outputs": [],
      "source": [
        "documents = search_df.to_json(\"temp.json\", orient=\"records\", index=True)\n",
        "with open(\"temp.json\", \"r\") as file:\n",
        "    documents = json.load(file)\n",
        "documents[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1694819072660
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# batch our docs into manageable sizes for cog services\n",
        "batches = np.array_split(documents, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694819229764
        }
      },
      "outputs": [],
      "source": [
        "search_client = SearchClient(\n",
        "    endpoint=service_endpoint, index_name=index_name, credential=credential\n",
        ")\n",
        "for batch in batches:\n",
        "    result = search_client.upload_documents(documents=batch.tolist())\n",
        "print(f\"Uploaded {len(documents)} documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Finally, run your search query.\n",
        "This example query combines both hybrid search (combining vector and text search), and semantic search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694819435626
        }
      },
      "outputs": [],
      "source": [
        "# Semantic Search\n",
        "query = \"What information does the introduction section contain?\"\n",
        "\n",
        "search_client = SearchClient(service_endpoint, index_name, credential=credential)\n",
        "vector = Vector(value=generate_embeddings(query), k=3, fields=\"Generated_embedding\")\n",
        "\n",
        "results = search_client.search(\n",
        "    search_text=query,\n",
        "    vectors=[vector],\n",
        "    select=[\"FileName\", \"Content\"],\n",
        "    query_type=\"semantic\",\n",
        "    query_language=\"en-us\",\n",
        "    semantic_configuration_name=\"my-semantic-config\",\n",
        "    query_caption=\"extractive\",\n",
        "    query_answer=\"extractive\",\n",
        "    top=5,\n",
        ")\n",
        "\n",
        "semantic_answers = results.get_answers()\n",
        "for answer in semantic_answers:\n",
        "    if answer.highlights:\n",
        "        print(f\"Semantic Answer: {answer.highlights}\")\n",
        "    else:\n",
        "        print(f\"Semantic Answer: {answer.text}\")\n",
        "    print(f\"Semantic Answer Score: {answer.score}\\n\")\n",
        "\n",
        "for result in results:\n",
        "    print(f\"Document: {result['FileName']}\")\n",
        "    print(f\"Score: {result['@search.score']}\")\n",
        "    print(f\"Content: {result['Content']}\")\n",
        "\n",
        "    captions = result[\"@search.captions\"]\n",
        "    if captions:\n",
        "        caption = captions[0]\n",
        "        if caption.highlights:\n",
        "            print(f\"Caption: {caption.highlights}\\n\")\n",
        "        else:\n",
        "            print(f\"Caption: {caption.text}\\n\")"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "formats": "ipynb,py:percent"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
