#!/usr/bin/env bash
set -euo pipefail

# Deploy Helm chart generated by Aspire 9.4 publish.
# This script deploys the Kubernetes workloads after Azure infrastructure is ready.
# Usage: build/scripts/build-modern-helm-deploy.sh <publish-output-dir> <release> <namespace> [values.yaml]

OUT_DIR=${1:-out/publish}
RELEASE=${2:?Missing release name}
NAMESPACE=${3:?Missing namespace}
VALUES_FILE=${4:-}

# Check for required tools
if ! command -v kubectl >/dev/null 2>&1; then
    echo "[modern] Error: kubectl is not installed" >&2
    echo "[modern] Install from: https://kubernetes.io/docs/tasks/tools/" >&2
    exit 1
fi

if ! command -v helm >/dev/null 2>&1; then
    echo "[modern] Error: Helm is not installed" >&2
    echo "[modern] Install from: https://helm.sh/docs/intro/install/" >&2
    exit 1
fi

echo "[modern] Deploying Kubernetes workloads using Helm"
echo "[modern] Output directory: $OUT_DIR"
echo "[modern] Release: $RELEASE"
echo "[modern] Namespace: $NAMESPACE"

# Check if Helm chart exists in Aspire 9.4 output structure
if [[ ! -f "$OUT_DIR/Chart.yaml" ]]; then
  echo "[modern] Error: No Helm chart found at $OUT_DIR/Chart.yaml" >&2
  echo "[modern] Ensure 'aspire publish' has been run successfully" >&2
  exit 1
fi

# Ensure namespace exists
echo "[modern] Ensuring namespace: $NAMESPACE"
kubectl get ns "$NAMESPACE" >/dev/null 2>&1 || kubectl create ns "$NAMESPACE"

# Load Azure deployment outputs if available
AZURE_OUTPUTS_FILE="${AZURE_OUTPUT_FILE:-azure-outputs.env}"
if [[ -f "$AZURE_OUTPUTS_FILE" ]]; then
  echo "[modern] Loading Azure deployment outputs from: $AZURE_OUTPUTS_FILE"
  set -a  # automatically export all variables
  source "$AZURE_OUTPUTS_FILE"
  set +a  # disable automatic export
else
  echo "[modern] No Azure outputs file found at: $AZURE_OUTPUTS_FILE"
fi

# Get Azure Container Registry information if available
ACR_LOGIN_SERVER="${AZURE_CONTAINER_REGISTRY_ENDPOINT:-}"
if [[ -z "$ACR_LOGIN_SERVER" ]] && [[ -n "${AZURE_OUTPUT_AZURE_CONTAINER_REGISTRY_ENDPOINT:-}" ]]; then
  ACR_LOGIN_SERVER="${AZURE_OUTPUT_AZURE_CONTAINER_REGISTRY_ENDPOINT}"
fi

# If still no ACR endpoint, try to find it from existing ACR in the resource group
if [[ -z "$ACR_LOGIN_SERVER" ]]; then
  echo "[modern] No ACR endpoint provided, attempting to discover from resource group..."
  DISCOVERED_ACR=$(az acr list -g "${AKS_RESOURCE_GROUP:-rg-greenlight-adodev}" --query "[0].loginServer" -o tsv 2>/dev/null || echo "")
  if [[ -n "$DISCOVERED_ACR" ]]; then
    ACR_LOGIN_SERVER="$DISCOVERED_ACR"
    echo "[modern] Discovered ACR: $ACR_LOGIN_SERVER"
  else
    echo "[modern] WARNING: Could not discover ACR in resource group ${AKS_RESOURCE_GROUP:-rg-greenlight-adodev}"
  fi
fi

# BUILD_BUILDNUMBER is MANDATORY for production deployments
if [[ -z "${BUILD_BUILDNUMBER:-}" ]]; then
  echo "[modern] ❌ ERROR: BUILD_BUILDNUMBER not set - deployment cannot proceed"
  echo "[modern] Production deployments require specific image tags, not 'latest'"
  echo "[modern] Pipeline must provide BUILD_BUILDNUMBER environment variable"
  exit 1
fi

# Debug environment variable status
echo "[modern] Environment variable debug:"
echo "  AZURE_CONTAINER_REGISTRY_ENDPOINT: '${AZURE_CONTAINER_REGISTRY_ENDPOINT:-unset}'"
echo "  AZURE_OUTPUT_AZURE_CONTAINER_REGISTRY_ENDPOINT: '${AZURE_OUTPUT_AZURE_CONTAINER_REGISTRY_ENDPOINT:-unset}'"
echo "  BUILD_BUILDNUMBER: '${BUILD_BUILDNUMBER:-unset}'"
echo "  BUILD_BUILDID: '${BUILD_BUILDID:-unset}'"
echo "  Resolved ACR_LOGIN_SERVER: '${ACR_LOGIN_SERVER:-unset}'"

# Force ACR discovery if not already set (since ACR exists independently of Aspire)
if [[ -z "$ACR_LOGIN_SERVER" ]]; then
  echo "[modern] Forcing ACR discovery since ACR is not created by Aspire..."
  ACR_LOGIN_SERVER=$(az acr list -g "${AKS_RESOURCE_GROUP:-rg-greenlight-adodev}" --query "[0].loginServer" -o tsv 2>/dev/null || echo "")
  if [[ -n "$ACR_LOGIN_SERVER" ]]; then
    echo "[modern] ✅ Found ACR: $ACR_LOGIN_SERVER"
  else
    echo "[modern] ❌ ERROR: No ACR found in resource group ${AKS_RESOURCE_GROUP:-rg-greenlight-adodev}"
  fi
fi

# Prepare Helm values overrides
echo "[modern] Preparing Helm values..."
OVERRIDE_VALUES_FILE=$(mktemp)
COMMON_CONFIG=$(mktemp)
PROCESSED_VALUES_FILE=$(mktemp)
CONNECTION_STRINGS_FILE=$(mktemp)
trap "rm -f $OVERRIDE_VALUES_FILE $COMMON_CONFIG $PROCESSED_VALUES_FILE $CONNECTION_STRINGS_FILE" EXIT

# Use the existing Aspire-generated values.yaml as the base
ASPIRE_VALUES_FILE="$OUT_DIR/values.yaml"
if [[ ! -f "$ASPIRE_VALUES_FILE" ]]; then
  echo "[modern] ERROR: No Aspire values.yaml found at $ASPIRE_VALUES_FILE" >&2
  exit 1
fi

echo "[modern] Using Aspire values from: $ASPIRE_VALUES_FILE"

# Create a copy of values.yaml to process placeholders
cp "$ASPIRE_VALUES_FILE" "$PROCESSED_VALUES_FILE"

# Replace Aspire placeholders with actual Azure deployment outputs
echo "[modern] Replacing Aspire placeholders with Azure deployment outputs..."
if env | grep -q "^AZURE_OUTPUT_"; then
  # Debug: Check first if file has placeholders before replacement
  echo "[modern] Checking for placeholders in values file..."
  grep -o "{[^}]*\.outputs\.[^}]*}" "$PROCESSED_VALUES_FILE" | head -5 || echo "[modern] No placeholders found initially"

  # Function to safely replace placeholders (handles all special characters)
  replace_placeholder() {
    local placeholder="$1"
    local replacement="$2"
    local file="$3"

    # For EventHub endpoints, remove the port number (not needed in connection strings)
    if [[ "$placeholder" == *"eventhub"* ]] && [[ "$replacement" == *":443/"* ]]; then
      replacement="${replacement%:443/}"  # Remove :443/ from the end
      echo "[modern] Removed port :443 from EventHub endpoint"
    fi

    # Use awk for safe replacement - it handles all special characters properly
    awk -v find="$placeholder" -v repl="$replacement" '
      {
        # Use index to find position, then reconstruct string
        # This avoids regex interpretation issues
        while ((i = index($0, find)) > 0) {
          $0 = substr($0, 1, i-1) repl substr($0, i+length(find))
        }
        print
      }
    ' "$file" > "$file.tmp" && mv "$file.tmp" "$file"
  }

  # First, handle specific known mappings for clarity
  for var in $(env | grep "^AZURE_OUTPUT_" | cut -d= -f1); do
    VALUE="${!var}"

    # Do not print secret values
    echo "[modern] Processing $var (value redacted)"

    # Map outputs to their Aspire placeholder names and replace
    case "$var" in
      AZURE_OUTPUT_sqlServerFqdn)
        replace_placeholder "{sqldocgen.outputs.sqlServerFqdn}" "$VALUE" "$PROCESSED_VALUES_FILE"
        echo "[modern] Replaced {sqldocgen.outputs.sqlServerFqdn}"
        ;;
      AZURE_OUTPUT_managedRedisHost)
        replace_placeholder "{managed-redis.outputs.managedRedisHost}" "$VALUE" "$PROCESSED_VALUES_FILE"
        echo "[modern] Replaced {managed-redis.outputs.managedRedisHost}"
        ;;
      AZURE_OUTPUT_managedRedisPort)
        replace_placeholder "{managed-redis.outputs.managedRedisPort}" "$VALUE" "$PROCESSED_VALUES_FILE"
        echo "[modern] Replaced {managed-redis.outputs.managedRedisPort}"
        ;;
      AZURE_OUTPUT_aiSearchConnectionString)
        replace_placeholder "{aiSearch.outputs.connectionString}" "$VALUE" "$PROCESSED_VALUES_FILE"
        echo "[modern] Replaced {aiSearch.outputs.connectionString}"
        ;;
      AZURE_OUTPUT_signalrHostName)
        # SignalR is disabled - using Redis backplane instead
        echo "[modern] Skipping SignalR hostname (using Redis backplane)"
        ;;
      AZURE_OUTPUT_docingBlobEndpoint)
        replace_placeholder "{docing.outputs.blobEndpoint}" "$VALUE" "$PROCESSED_VALUES_FILE"
        echo "[modern] Replaced {docing.outputs.blobEndpoint}"
        ;;
      AZURE_OUTPUT_docingTableEndpoint)
        replace_placeholder "{docing.outputs.tableEndpoint}" "$VALUE" "$PROCESSED_VALUES_FILE"
        echo "[modern] Replaced {docing.outputs.tableEndpoint}"
        ;;
      AZURE_OUTPUT_docingQueueEndpoint)
        replace_placeholder "{docing.outputs.queueEndpoint}" "$VALUE" "$PROCESSED_VALUES_FILE"
        echo "[modern] Replaced {docing.outputs.queueEndpoint}"
        ;;
      AZURE_OUTPUT_orleansStorageBlobEndpoint)
        replace_placeholder "{orleans-storage.outputs.blobEndpoint}" "$VALUE" "$PROCESSED_VALUES_FILE"
        echo "[modern] Replaced {orleans-storage.outputs.blobEndpoint}"
        ;;
      AZURE_OUTPUT_orleansStorageTableEndpoint)
        replace_placeholder "{orleans-storage.outputs.tableEndpoint}" "$VALUE" "$PROCESSED_VALUES_FILE"
        echo "[modern] Replaced {orleans-storage.outputs.tableEndpoint}"
        ;;
      AZURE_OUTPUT_orleansStorageQueueEndpoint)
        replace_placeholder "{orleans-storage.outputs.queueEndpoint}" "$VALUE" "$PROCESSED_VALUES_FILE"
        echo "[modern] Replaced {orleans-storage.outputs.queueEndpoint}"
        ;;
      AZURE_OUTPUT_orleansStorageBlobConnectionString)
        # Use full connection string for Orleans blob storage
        echo "[modern] Using Orleans blob storage connection string (with authentication)"
        # Replace the endpoint-only values with full connection strings
        sed -i "s|ConnectionStrings__blob-orleans:.*|ConnectionStrings__blob-orleans: \"$VALUE\"|g" "$PROCESSED_VALUES_FILE"
        ;;
      AZURE_OUTPUT_orleansClusteringConnectionString|AZURE_OUTPUT_orleansStorageTableConnectionString)
        # Use full connection string for Orleans clustering
        echo "[modern] Using Orleans clustering connection string (with authentication)"
        sed -i "s|ConnectionStrings__clustering:.*|ConnectionStrings__clustering: \"$VALUE\"|g" "$PROCESSED_VALUES_FILE"
        ;;
      AZURE_OUTPUT_orleansCheckpointingConnectionString)
        # Use full connection string for Orleans checkpointing
        echo "[modern] Using Orleans checkpointing connection string (with authentication)"
        sed -i "s|ConnectionStrings__checkpointing:.*|ConnectionStrings__checkpointing: \"$VALUE\"|g" "$PROCESSED_VALUES_FILE"
        ;;
      AZURE_OUTPUT_appInsightsConnectionString)
        replace_placeholder "{insights.outputs.appInsightsConnectionString}" "$VALUE" "$PROCESSED_VALUES_FILE"
        echo "[modern] Replaced {insights.outputs.appInsightsConnectionString}"
        ;;
      AZURE_OUTPUT_eventHubsEndpoint)
        replace_placeholder "{eventhub.outputs.eventHubsEndpoint}" "$VALUE" "$PROCESSED_VALUES_FILE"
        echo "[modern] Replaced {eventhub.outputs.eventHubsEndpoint}"
        ;;
      *)
        # For any unmapped outputs, log them
        echo "[modern] Available but unmapped: $var = [REDACTED]"
        ;;
    esac
  done

  # Check for any remaining placeholders and warn
  if grep -q "{.*\.outputs\." "$PROCESSED_VALUES_FILE"; then
    echo "[modern] WARNING: Some placeholders were not replaced:"
    grep -o "{[^}]*\.outputs\.[^}]*}" "$PROCESSED_VALUES_FILE" | sort -u | while read placeholder; do
      echo "[modern]   - $placeholder"
    done
  fi
else
  echo "[modern] WARNING: No AZURE_OUTPUT_ variables found - placeholders will not be replaced"
  echo "[modern] This likely means Azure deployment was skipped or failed"
fi

# Use the processed values file as the base
ASPIRE_VALUES_FILE="$PROCESSED_VALUES_FILE"

echo "[modern] Using processed values from: $ASPIRE_VALUES_FILE"

# Extract connection strings from values and prepare Kubernetes secrets
echo "[modern] Preparing Kubernetes secrets for connection strings..."

# Extract all ConnectionStrings__ entries from the processed values (grep returns 0 even if no match)
if grep -E "ConnectionStrings__[^:]+:" "$PROCESSED_VALUES_FILE" > /dev/null 2>&1; then
  grep -E "ConnectionStrings__[^:]+:" "$PROCESSED_VALUES_FILE" | while IFS=: read -r key value; do
    # Clean up the key and value
    KEY=$(echo "$key" | sed 's/^[[:space:]]*//' | sed 's/[[:space:]]*$//')
    VALUE=$(echo "$value" | sed 's/^[[:space:]]*"//' | sed 's/"[[:space:]]*$//' | sed 's/^[[:space:]]*//')

    # Skip if value still has placeholders
    if [[ "$VALUE" == *"{"*"}"* ]]; then
      echo "[modern] WARNING: Skipping $KEY - still has placeholders: $VALUE"
      continue
    fi

    # Add to connection strings file
    echo "${KEY}=${VALUE}" >> "$CONNECTION_STRINGS_FILE"
  done
else
  echo "[modern] No connection strings found in values file"
fi

# Generate or reuse Redis passwords for containerized instances
# Prefer stable values from existing cluster resources to avoid auth mismatch
get_existing_password() {
  local kind="$1" # redis | redis-signalr
  local ns="$NAMESPACE"
  # 1) Try configmap first
  if kubectl get cm ${kind}-config -n "$ns" >/dev/null 2>&1; then
    local val=$(kubectl get cm ${kind}-config -n "$ns" -o jsonpath='{.data.REDIS_PASSWORD}' 2>/dev/null || echo "")
    if [[ -n "$val" ]]; then echo "$val"; return 0; fi
  fi
  # 2) Try api-main-secrets (connection strings)
  if kubectl get secret api-main-secrets -n "$ns" >/dev/null 2>&1; then
    local key="ConnectionStrings__redis"
    [[ "$kind" == "redis-signalr" ]] && key="ConnectionStrings__redis-signalr"
    local b64=$(kubectl get secret api-main-secrets -n "$ns" -o jsonpath="{.data.$key}" 2>/dev/null || echo "")
    if [[ -n "$b64" ]]; then
      local decoded=$(python3 - "$b64" << 'PY'
import base64,sys
s=base64.b64decode(sys.argv[1]).decode('utf-8')
print(s)
PY
)
      # extract after password=
      local pw=${decoded##*password=}
      if [[ -n "$pw" && "$pw" != "$decoded" ]]; then echo "$pw"; return 0; fi
    fi
  fi
  # 3) Fallback generate new
  openssl rand -hex 32
}

REDIS_PASSWORD="${REDIS_PASSWORD:-$(get_existing_password redis)}"
REDIS_SIGNALR_PASSWORD="${REDIS_SIGNALR_PASSWORD:-$(get_existing_password redis-signalr)}"

# Add Redis connection strings for containerized instances (to be consumed by apps)
echo "[modern] Adding containerized Redis connection strings (redacted)..."
echo "ConnectionStrings__redis=redis:6379,password=${REDIS_PASSWORD}" >> "$CONNECTION_STRINGS_FILE"
echo "ConnectionStrings__redis-signalr=redis-signalr:6379,password=${REDIS_SIGNALR_PASSWORD}" >> "$CONNECTION_STRINGS_FILE"

# Note: Orleans storage connection strings should be endpoint-only when using workload identity
# The DefaultAzureCredential in the application will handle authentication

# Add OpenAI connection string if provided (do this BEFORE creating the secret)
if [[ -n "${PVICO_OPENAI_CONNECTIONSTRING:-}" ]]; then
  echo "[modern] Adding OpenAI connection string..."
  echo "ConnectionStrings__openai-planner=${PVICO_OPENAI_CONNECTIONSTRING}" >> "$CONNECTION_STRINGS_FILE"
  echo "[modern] Added OpenAI connection string to connection strings file"
fi

# Create the secret if we have any connection strings
if [[ -s "$CONNECTION_STRINGS_FILE" ]]; then
  # Remove duplicate keys from connection strings file (keep last occurrence)
  echo "[modern] Processing connection strings for uniqueness..."
  awk -F= '!seen[$1]++ {keys[++n]=$1} {values[$1]=$0} END {for(i=1;i<=n;i++) print values[keys[i]]}' "$CONNECTION_STRINGS_FILE" > "${CONNECTION_STRINGS_FILE}.tmp"
  mv "${CONNECTION_STRINGS_FILE}.tmp" "$CONNECTION_STRINGS_FILE"

  echo "[modern] Creating greenlight-connection-strings secret..."

  # Delete existing secret if it exists to avoid key conflicts
  kubectl delete secret greenlight-connection-strings --namespace="$NAMESPACE" --ignore-not-found=true

  # Create the new secret
  kubectl create secret generic greenlight-connection-strings \
    --from-env-file="$CONNECTION_STRINGS_FILE" \
    --namespace="$NAMESPACE" \
    --dry-run=client -o yaml | kubectl apply -f -
  echo "[modern] Connection strings secret created successfully"

  # The post-publish fixes will handle moving connection strings from ConfigMaps to Secrets
  echo "[modern] Connection strings will be moved from ConfigMaps to Secrets during post-publish fixes"
else
  echo "[modern] No valid connection strings found to create secret"
fi

# Start with empty override file - only add if we have actual overrides
echo "# Auto-generated overrides from deployment script" > "$OVERRIDE_VALUES_FILE"
HAS_OVERRIDES=false

# ACR configuration is MANDATORY for production deployments
if [[ -z "$ACR_LOGIN_SERVER" ]]; then
  echo "[modern] ❌ ERROR: No ACR endpoint configured - deployment cannot proceed"
  echo "[modern] Container images cannot be pulled without ACR authentication"
  echo "[modern] Expected:"
  echo "  - ACR should exist in resource group ${AKS_RESOURCE_GROUP:-rg-greenlight-adodev}"
  echo "  - Or provide AZURE_CONTAINER_REGISTRY_ENDPOINT environment variable"
  echo "[modern] Pipeline must fail to prevent broken deployment"
  exit 1
fi

echo "[modern] ✅ Configuring container images for ACR: $ACR_LOGIN_SERVER"

if [[ -n "$ACR_LOGIN_SERVER" ]]; then

  # Use build number for image tag, fallback to latest if not available
  # Convert dots to dashes for container registry compatibility
  if [[ -n "${BUILD_BUILDNUMBER:-}" ]]; then
    IMAGE_TAG="${BUILD_BUILDNUMBER//./-}"
  else
    IMAGE_TAG="latest"
  fi
  echo "[modern] Using image tag: $IMAGE_TAG (from BUILD_BUILDNUMBER: ${BUILD_BUILDNUMBER:-unset})"

  cat >> "$OVERRIDE_VALUES_FILE" <<EOF
parameters:
  db_setupmanager:
    db_setupmanager_image: "${ACR_LOGIN_SERVER}/db-setupmanager:${IMAGE_TAG}"
  api_main:
    api_main_image: "${ACR_LOGIN_SERVER}/api-main:${IMAGE_TAG}"
  mcp_server:
    mcp_server_image: "${ACR_LOGIN_SERVER}/mcp-server:${IMAGE_TAG}"
  silo:
    silo_image: "${ACR_LOGIN_SERVER}/silo:${IMAGE_TAG}"
  web_docgen:
    web_docgen_image: "${ACR_LOGIN_SERVER}/web-docgen:${IMAGE_TAG}"
  redis:
    redis_password: "${REDIS_PASSWORD}"
  redis_signalr:
    redis_signalr_password: "${REDIS_SIGNALR_PASSWORD}"

# Note: Redis passwords will be delivered via a dedicated Kubernetes Secret (not ConfigMap)
EOF

  # Ensure AKS-ACR integration for container image pulls
  echo "[modern] Configuring AKS-ACR integration..."
  ACR_NAME=$(echo "$ACR_LOGIN_SERVER" | cut -d'.' -f1)

  # Check if ACR is already attached to AKS
  if az aks check-acr --name "${AKS_CLUSTER_NAME:-aks-greenlight-adodev}" --resource-group "${AKS_RESOURCE_GROUP:-rg-greenlight-adodev}" --acr "$ACR_NAME" >/dev/null 2>&1; then
    echo "[modern] ACR integration already configured"
  else
    echo "[modern] Attaching ACR to AKS cluster for seamless image pulls..."
    if az aks update --name "${AKS_CLUSTER_NAME:-aks-greenlight-adodev}" --resource-group "${AKS_RESOURCE_GROUP:-rg-greenlight-adodev}" --attach-acr "$ACR_NAME" >/dev/null 2>&1; then
      echo "[modern] ✅ Successfully attached ACR $ACR_NAME to AKS cluster"
    else
      echo "[modern] ❌ ERROR: Failed to attach ACR to AKS cluster"
      echo "[modern] This will cause ImagePullBackOff errors - deployment cannot proceed"
      echo "[modern] Manual fix: az aks update --name ${AKS_CLUSTER_NAME:-aks-greenlight-adodev} --resource-group ${AKS_RESOURCE_GROUP:-rg-greenlight-adodev} --attach-acr $ACR_NAME"
      exit 1
    fi
  fi

  HAS_OVERRIDES=true
else
  echo "[modern] Warning: No ACR endpoint found - using local image references"
fi

# Check if we need to add configuration overrides
NEEDS_CONFIG_SECTION=false

# Add Azure resource connection strings from deployment outputs
if env | grep -q "^AZURE_OUTPUT_"; then
  echo "[modern] Adding Azure resource connection strings to configuration..."

  # Add config section header if this is the first config override
  if [[ "$NEEDS_CONFIG_SECTION" == "false" ]]; then
    echo "" >> "$OVERRIDE_VALUES_FILE"
    echo "config:" >> "$OVERRIDE_VALUES_FILE"
    NEEDS_CONFIG_SECTION=true
  fi

  # Common configuration for all services
  cat >> "$OVERRIDE_VALUES_FILE" <<EOF
  _common:
EOF

  # Add all Azure outputs as configuration
  # Connection strings are handled via Kubernetes Secrets, not ConfigMaps
  # So we don't add them to the override values file
  echo "[modern] Skipping connection strings in override values (using Secrets instead)"

  HAS_OVERRIDES=true
fi

# Add critical environment variables
echo "[modern] Adding critical environment variables..."

# Ensure config section exists
if [[ "$NEEDS_CONFIG_SECTION" == "false" ]]; then
  echo "" >> "$OVERRIDE_VALUES_FILE"
  echo "config:" >> "$OVERRIDE_VALUES_FILE"
  echo "  _common:" >> "$OVERRIDE_VALUES_FILE"
  NEEDS_CONFIG_SECTION=true
fi

# Add AZURE_CLIENT_ID from workload identity if available
if [[ -n "${WORKLOAD_IDENTITY_CLIENT_ID:-}" ]]; then
  echo "[modern] Adding AZURE_CLIENT_ID for workload identity"
  cat >> "$OVERRIDE_VALUES_FILE" <<EOF
    AZURE_CLIENT_ID: "${WORKLOAD_IDENTITY_CLIENT_ID}"
EOF
  HAS_OVERRIDES=true
fi

# Add Azure AD configuration for authentication
# PVICO_ENTRA_CREDENTIALS contains the complete AzureAd configuration as JSON
if [[ -n "${PVICO_ENTRA_CREDENTIALS:-}" ]]; then
  echo "[modern] Adding Azure AD configuration from PVICO_ENTRA_CREDENTIALS"

  # Parse the JSON and add each field as AzureAd__ prefixed config
  # The JSON should contain fields like TenantId, ClientId, Instance, Domain, Scopes, etc.
  echo "$PVICO_ENTRA_CREDENTIALS" | jq -r 'to_entries | .[] | "    AzureAd__\(.key): \"\(.value)\""' >> "$OVERRIDE_VALUES_FILE"

  HAS_OVERRIDES=true
elif [[ -n "${AZURE_AD_TENANT_ID:-}" ]]; then
  # Fallback: Use individual Azure AD variables if PVICO_ENTRA_CREDENTIALS not provided
  echo "[modern] Adding Azure AD configuration from individual variables"

  # Use workload identity client ID as default if no explicit Azure AD client ID is provided
  DEFAULT_CLIENT_ID="${AZURE_AD_CLIENT_ID:-${WORKLOAD_IDENTITY_CLIENT_ID:-}}"

  # If still no client ID, use the workload identity as a fallback
  # This allows the app to start even without a proper app registration
  if [[ -z "$DEFAULT_CLIENT_ID" ]]; then
    echo "[modern] WARNING: No Azure AD Client ID found - authentication will be disabled"
    # Use a special value that the app can detect to disable auth
    DEFAULT_CLIENT_ID="disabled"
  fi

  cat >> "$OVERRIDE_VALUES_FILE" <<EOF
    AzureAd__TenantId: "${AZURE_AD_TENANT_ID}"
    AzureAd__ClientId: "${DEFAULT_CLIENT_ID}"
    AzureAd__Instance: "${AZURE_AD_INSTANCE:-https://login.microsoftonline.com/}"
    AzureAd__Scopes: "${AZURE_AD_SCOPES:-api://greenlight/.default}"
EOF
  HAS_OVERRIDES=true
fi

# Add GREENLIGHT_PRODUCTION flag for production deployments
# This is always true for K8s deployments (non-local)
echo "[modern] Setting GREENLIGHT_PRODUCTION=true"
cat >> "$OVERRIDE_VALUES_FILE" <<EOF
    GREENLIGHT_PRODUCTION: "true"
EOF

# Configure DataProtection to use Redis (matching ACA deployment)
# This is critical for scale-out scenarios
echo "[modern] Setting DataProtection directory for Redis"
cat >> "$OVERRIDE_VALUES_FILE" <<EOF
    DOTNET_ReadOnlyDataProtectionKeyDirectory: "false"
EOF

# Orleans storage configuration is already in the published values.yaml
# We just need to ensure it doesn't get overridden with empty values
HAS_OVERRIDES=true

# OpenAI connection string is now handled earlier with other connection strings
# This prevents duplicate key issues and ensures it's included in the secret

# Handle Azure AD Client Secret separately if it's in PVICO_ENTRA_CREDENTIALS
# The ClientSecret should be stored as a Kubernetes secret, not in ConfigMap
if [[ -n "${PVICO_ENTRA_CREDENTIALS:-}" ]]; then
  CLIENT_SECRET=$(echo "$PVICO_ENTRA_CREDENTIALS" | jq -r '.ClientSecret // empty')

  if [[ -n "$CLIENT_SECRET" ]]; then
    echo "[modern] Creating Azure AD client secret in Kubernetes..."
    kubectl create secret generic greenlight-auth \
      --from-literal="client-secret=${CLIENT_SECRET}" \
      --namespace="$NAMESPACE" \
      --dry-run=client -o yaml | kubectl apply -f -

    # Reference the secret in the config
    cat >> "$OVERRIDE_VALUES_FILE" <<EOF
    AzureAd__ClientSecret: ""  # Clear from ConfigMap
    AzureAd__ClientSecretRef__Name: "greenlight-auth"
    AzureAd__ClientSecretRef__Key: "client-secret"
EOF
  fi
fi

# Add hostname override if provided
if [[ -n "${HOSTNAME_OVERRIDE:-}" ]]; then
  echo "[modern] Adding hostname override configuration..."

  # Parse JSON and add as configuration
  WEB_URL=$(echo "$HOSTNAME_OVERRIDE" | jq -r '.WebApplicationUrl // empty')
  API_URL=$(echo "$HOSTNAME_OVERRIDE" | jq -r '.ApiBaseUrl // empty')

  if [[ -n "$WEB_URL" ]] || [[ -n "$API_URL" ]]; then
    # Add config section header if this is the first config override
    if [[ "$NEEDS_CONFIG_SECTION" == "false" ]]; then
      echo "" >> "$OVERRIDE_VALUES_FILE"
      echo "config:" >> "$OVERRIDE_VALUES_FILE"
      echo "  _common:" >> "$OVERRIDE_VALUES_FILE"
      NEEDS_CONFIG_SECTION=true
    fi

    if [[ -n "$WEB_URL" ]]; then
      cat >> "$OVERRIDE_VALUES_FILE" <<EOF
    ServiceConfiguration__HostNameOverride__WebApplicationUrl: "${WEB_URL}"
EOF
    fi

    if [[ -n "$API_URL" ]]; then
      cat >> "$OVERRIDE_VALUES_FILE" <<EOF
    ServiceConfiguration__HostNameOverride__ApiBaseUrl: "${API_URL}"
EOF
    fi

    HAS_OVERRIDES=true
  fi
fi

# Add Azure Maps key if provided
if [[ -n "${PVICO_AZUREMAPS_KEY:-}" ]]; then
  echo "[modern] Adding Azure Maps configuration..."

  # Add config section header if this is the first config override
  if [[ "$NEEDS_CONFIG_SECTION" == "false" ]]; then
    echo "" >> "$OVERRIDE_VALUES_FILE"
    echo "config:" >> "$OVERRIDE_VALUES_FILE"
    echo "  _common:" >> "$OVERRIDE_VALUES_FILE"
    NEEDS_CONFIG_SECTION=true
  fi

  cat >> "$OVERRIDE_VALUES_FILE" <<EOF
    ServiceConfiguration__AzureMaps__Key: "${PVICO_AZUREMAPS_KEY}"
EOF
  HAS_OVERRIDES=true
fi

# Apply configuration overrides to all services if we have any
if [[ "$HAS_OVERRIDES" == "true" ]] && [[ "$NEEDS_CONFIG_SECTION" == "true" ]]; then
  echo "[modern] Applying configuration overrides to all services..."

  # Extract common configuration from override file

  if grep -A 1000 "^  _common:" "$OVERRIDE_VALUES_FILE" | grep "^    " > "$COMMON_CONFIG"; then
    echo "[modern] Found common configuration to apply to services"

    # Apply to each service - but preserve existing Orleans config for silo
    for service in db_setupmanager api_main mcp_server silo web_docgen; do
      echo "  ${service}:" >> "$OVERRIDE_VALUES_FILE"

      # For silo, we need to preserve Orleans GrainStorage configuration that Aspire generated
      if [[ "$service" == "silo" ]]; then
        # Add common config
        cat "$COMMON_CONFIG" >> "$OVERRIDE_VALUES_FILE"
        # Also explicitly preserve Orleans GrainStorage config from Aspire
        cat >> "$OVERRIDE_VALUES_FILE" <<EOF
    Orleans__GrainStorage__blob-orleans__ProviderType: "AzureBlobStorage"
    Orleans__GrainStorage__blob-orleans__ServiceKey: "blob-orleans"
EOF
      else
        cat "$COMMON_CONFIG" >> "$OVERRIDE_VALUES_FILE"
      fi
    done
  else
    echo "[modern] No common configuration found in overrides"
  fi
else
  echo "[modern] No configuration overrides to apply"
fi

echo "[modern] Using Aspire base values (not printed to avoid sensitive leakage)"

if [[ "$HAS_OVERRIDES" == "true" ]]; then
  echo "[modern] Generated override values (redacted)"
fi

# Note: Aspire doesn't generate ConfigMap templates, we'll create them after deployment
echo "[modern] Note: ConfigMaps will be created post-deployment (Aspire doesn't generate templates)"

# Add workload identity service account and environment if configured
if [[ -n "${WORKLOAD_IDENTITY_CLIENT_ID:-}" ]]; then
  echo "[modern] Adding workload identity service account to deployments..."
  WI_SCRIPT="$(dirname "$0")/pipeline-internal/build-modern-add-wi-serviceaccount.sh"
  if [[ -f "$WI_SCRIPT" ]]; then
    bash "$WI_SCRIPT" "$OUT_DIR" "$NAMESPACE" "${WORKLOAD_IDENTITY_SERVICE_ACCOUNT:-greenlight-app}"
  else
    echo "[modern] WARNING: Workload identity script not found at $WI_SCRIPT"
  fi

  # Inject workload identity environment variables
  echo "[modern] Injecting workload identity environment variables..."
  WI_ENV_SCRIPT="$(dirname "$0")/pipeline-internal/build-modern-inject-workload-identity-env.sh"
  if [[ -f "$WI_ENV_SCRIPT" ]]; then
    bash "$WI_ENV_SCRIPT" "$OUT_DIR" "$NAMESPACE" "${WORKLOAD_IDENTITY_SERVICE_ACCOUNT:-greenlight-app}"
  else
    echo "[modern] WARNING: Workload identity environment injection script not found"
  fi
fi

# Perform Helm deployment
echo "[modern] Deploying Helm chart..."

# Use Aspire values as base, add overrides if any exist
if [[ "$HAS_OVERRIDES" == "true" ]]; then
  HELM_COMMAND="helm upgrade --install $RELEASE $OUT_DIR -n $NAMESPACE -f $ASPIRE_VALUES_FILE -f $OVERRIDE_VALUES_FILE"
  echo "[modern] Using base values + overrides"
else
  HELM_COMMAND="helm upgrade --install $RELEASE $OUT_DIR -n $NAMESPACE -f $ASPIRE_VALUES_FILE"
  echo "[modern] Using base values only"
fi

# Add custom values file if provided
if [[ -n "$VALUES_FILE" ]] && [[ -f "$VALUES_FILE" ]]; then
  echo "[modern] Using additional custom values from: $VALUES_FILE"
  HELM_COMMAND="$HELM_COMMAND -f $VALUES_FILE"
fi

# Add Kubernetes resource configuration if provided
if [[ -n "${KUBERNETES_RESOURCES_CONFIG:-}" ]]; then
  echo "[modern] Applying Kubernetes resource configuration..."

  # Parse JSON and add resource limits/requests for each service
  for service in api-main web-main silo mcp-server db-setupmanager; do
    SERVICE_KEY=$(echo "$service" | tr '-' '_')

    # Extract resource configuration for this service
    RESOURCES=$(echo "$KUBERNETES_RESOURCES_CONFIG" | jq -r ".\"${service}\" // empty")

    if [[ -n "$RESOURCES" ]] && [[ "$RESOURCES" != "null" ]]; then
      # Add resource requests
      REQ_MEM=$(echo "$RESOURCES" | jq -r '.resources.requests.memory // empty')
      REQ_CPU=$(echo "$RESOURCES" | jq -r '.resources.requests.cpu // empty')
      LIM_MEM=$(echo "$RESOURCES" | jq -r '.resources.limits.memory // empty')
      LIM_CPU=$(echo "$RESOURCES" | jq -r '.resources.limits.cpu // empty')

      if [[ -n "$REQ_MEM" ]]; then
        HELM_COMMAND="$HELM_COMMAND --set ${SERVICE_KEY}.resources.requests.memory=${REQ_MEM}"
      fi
      if [[ -n "$REQ_CPU" ]]; then
        HELM_COMMAND="$HELM_COMMAND --set ${SERVICE_KEY}.resources.requests.cpu=${REQ_CPU}"
      fi
      if [[ -n "$LIM_MEM" ]]; then
        HELM_COMMAND="$HELM_COMMAND --set ${SERVICE_KEY}.resources.limits.memory=${LIM_MEM}"
      fi
      if [[ -n "$LIM_CPU" ]]; then
        HELM_COMMAND="$HELM_COMMAND --set ${SERVICE_KEY}.resources.limits.cpu=${LIM_CPU}"
      fi

      # Add autoscaling configuration if present
      MIN_REPLICAS=$(echo "$RESOURCES" | jq -r '.autoscaling.minReplicas // empty')
      MAX_REPLICAS=$(echo "$RESOURCES" | jq -r '.autoscaling.maxReplicas // empty')

      if [[ -n "$MIN_REPLICAS" ]]; then
        HELM_COMMAND="$HELM_COMMAND --set ${SERVICE_KEY}.autoscaling.enabled=true"
        HELM_COMMAND="$HELM_COMMAND --set ${SERVICE_KEY}.autoscaling.minReplicas=${MIN_REPLICAS}"
      fi
      if [[ -n "$MAX_REPLICAS" ]]; then
        HELM_COMMAND="$HELM_COMMAND --set ${SERVICE_KEY}.autoscaling.maxReplicas=${MAX_REPLICAS}"
      fi
    fi
  done
fi

# Execute Helm deployment
echo "[modern] Executing: $HELM_COMMAND"
eval "$HELM_COMMAND"

# Handle Azure role assignment propagation for workload identity authentication
echo "[modern] Handling Azure role assignment propagation delays..."

# Wait for critical Orleans-dependent services to start, then force token refresh if needed
ORLEANS_SERVICES=("silo-deployment" "api-main-deployment")
MAX_WAIT_TIME=300  # 5 minutes maximum wait
PROPAGATION_DELAY=30  # Initial delay for role propagation

echo "[modern] Waiting ${PROPAGATION_DELAY}s for Azure role assignments to propagate..."
sleep $PROPAGATION_DELAY

for service in "${ORLEANS_SERVICES[@]}"; do
  echo "[modern] Checking $service startup..."

  # Wait up to MAX_WAIT_TIME for the deployment to exist and have at least one pod
  end_time=$((SECONDS + MAX_WAIT_TIME))
  while [[ $SECONDS -lt $end_time ]]; do
    if kubectl get deployment "$service" -n "$NAMESPACE" >/dev/null 2>&1; then
      # Check if pods are in CrashLoopBackOff due to auth issues
      pod_status=$(kubectl get pods -n "$NAMESPACE" -l "app.kubernetes.io/component=${service%-deployment}" -o jsonpath='{.items[0].status.phase}' 2>/dev/null || echo "")
      container_statuses=$(kubectl get pods -n "$NAMESPACE" -l "app.kubernetes.io/component=${service%-deployment}" -o jsonpath='{.items[0].status.containerStatuses[0].state}' 2>/dev/null || echo "")

      # If pod is in CrashLoopBackOff or has auth-related failures, force restart
      if [[ "$container_statuses" == *"CrashLoopBackOff"* ]] || kubectl logs -n "$NAMESPACE" -l "app.kubernetes.io/component=${service%-deployment}" --tail=20 2>/dev/null | grep -q "AuthorizationFailure\|Forbidden\|401\|403"; then
        echo "[modern] Detected authentication failures in $service, forcing pod restart for fresh tokens..."
        kubectl delete pods -n "$NAMESPACE" -l "app.kubernetes.io/component=${service%-deployment}" --ignore-not-found=true

        # Wait for new pod to start
        echo "[modern] Waiting for $service to restart with fresh tokens..."
        kubectl wait --for=condition=Ready pod -l "app.kubernetes.io/component=${service%-deployment}" -n "$NAMESPACE" --timeout=120s || {
          echo "[modern] Warning: $service did not become ready within 120s after token refresh"
        }
      elif [[ "$pod_status" == "Running" ]]; then
        echo "[modern] $service is running successfully"
        break
      fi
    fi

    echo "[modern] Waiting for $service to be available..."
    sleep 10
  done

  if [[ $SECONDS -ge $end_time ]]; then
    echo "[modern] Warning: Timeout waiting for $service to start properly"
  fi
done

echo "[modern] Azure role propagation handling completed"

# Ensure Redis passwords are managed via a dedicated Secret and wired into statefulsets
echo "[modern] Applying Redis password secrets and patching statefulsets..."

# Create/update redis-auth secret with both passwords
kubectl delete secret redis-auth -n "$NAMESPACE" --ignore-not-found=true >/dev/null 2>&1 || true
kubectl create secret generic redis-auth -n "$NAMESPACE" \
  --from-literal=REDIS_PASSWORD="$REDIS_PASSWORD" \
  --from-literal=REDIS_SIGNALR_PASSWORD="$REDIS_SIGNALR_PASSWORD" \
  --dry-run=client -o yaml | kubectl apply -f -

# Patch helper to set env from secret for a statefulset's redis container
patch_redis_env() {
  local sts="$1" key="$2"
  kubectl patch statefulset "$sts" -n "$NAMESPACE" --type merge -p "$(cat <<'JSON'
{
  "spec": {
    "template": {
      "spec": {
        "containers": [
          {
            "name": "redis",
            "env": [
              {
                "name": "REDIS_PASSWORD",
                "valueFrom": { "secretKeyRef": { "name": "redis-auth", "key": "__KEY__" } }
              }
            ]
          }
        ]
      }
    }
  }
}
JSON
)" | sed "s/__KEY__/$key/g" >/dev/null 2>&1 || true
}

patch_redis_env "redis-statefulset" "REDIS_PASSWORD"
patch_redis_env "redis-signalr-statefulset" "REDIS_SIGNALR_PASSWORD"

# Create ConfigMaps since Aspire doesn't generate templates for them
echo "[modern] Creating ConfigMaps (Aspire doesn't generate templates)..."

# Function to create or update a ConfigMap
create_configmap() {
  local name=$1
  local namespace=$2

  echo "[modern]   Creating/updating ConfigMap: $name"

  # Start with base configuration
  kubectl create configmap "$name" \
    --from-literal="ASPNETCORE_ENVIRONMENT=Production" \
    --from-literal="DOTNET_ENVIRONMENT=Production" \
    -n "$namespace" --dry-run=client -o yaml | kubectl apply -f -
}

# Create ConfigMaps for each service
create_configmap "api-main-config" "$NAMESPACE"
create_configmap "silo-config" "$NAMESPACE"
create_configmap "web-docgen-config" "$NAMESPACE"
create_configmap "db-setupmanager-config" "$NAMESPACE"
create_configmap "mcp-server-config" "$NAMESPACE"

# Apply ConfigMap patches with required configuration
echo "[modern] Applying ConfigMap patches for missing Aspire template entries..."
FIX_SCRIPT="$(dirname "$0")/pipeline-internal/build-modern-fix-configmaps.sh"
if [[ -f "$FIX_SCRIPT" ]]; then
  # Pass all Azure output variables to the fix script
  export AZURE_AD_TENANT_ID="${AZURE_AD_TENANT_ID:-}"
  export AZURE_AD_CLIENT_ID="${AZURE_AD_CLIENT_ID:-}"
  export WORKLOAD_IDENTITY_CLIENT_ID="${WORKLOAD_IDENTITY_CLIENT_ID:-}"
  export PVICO_ENTRA_CREDENTIALS="${PVICO_ENTRA_CREDENTIALS:-}"

  # Export all Azure outputs for the fix script
  for var in $(env | grep "^AZURE_OUTPUT_" | cut -d= -f1); do
    export "$var"
  done

  bash "$FIX_SCRIPT" "$NAMESPACE"
else
  echo "[modern] Warning: ConfigMap fix script not found at $FIX_SCRIPT"
fi

# Wait for deployments to be ready
echo "[modern] Waiting for deployments to be ready..."
kubectl wait --for=condition=available --timeout=300s \
  deployment --all -n "$NAMESPACE" 2>/dev/null || true

# Show deployment status
echo ""
echo "========================================="
echo "Kubernetes Deployment Summary"
echo "========================================="
kubectl get deployments -n "$NAMESPACE"
echo ""
echo "Pods:"
kubectl get pods -n "$NAMESPACE"
echo ""
echo "Services:"
kubectl get services -n "$NAMESPACE"
echo ""

# Try to get the ingress/load balancer endpoint
INGRESS_IP=$(kubectl get svc -n "$NAMESPACE" -l "app.kubernetes.io/component=ingress" -o jsonpath='{.items[0].status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
if [[ -z "$INGRESS_IP" ]]; then
  # Try to find any LoadBalancer service
  INGRESS_IP=$(kubectl get svc -n "$NAMESPACE" --field-selector="spec.type=LoadBalancer" -o jsonpath='{.items[0].status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
fi

if [[ -n "$INGRESS_IP" ]]; then
  echo "Application endpoint: http://${INGRESS_IP}"
  echo ""
  echo "Note: Add this redirect URL to your Entra ID app registration:"
  echo "  https://${INGRESS_IP}/signin-oidc"
else
  echo "No external endpoint found yet - services may still be provisioning"
fi

echo "========================================="
echo "[modern] Helm deployment complete"
